#!/usr/bin/env python3
"""
5íšŒ ë°˜ë³µ ì‚¬ì´ë²„ë³´ì•ˆ ë„ë©”ì¸ LLM ë²¤ì¹˜ë§ˆí¬
ì‹¤ì œ ëª¨ë¸ë“¤ì„ í˜¸ì¶œí•˜ì—¬ ì„±ëŠ¥ì„ ì¸¡ì •í•©ë‹ˆë‹¤.
"""

import argparse
import json
import logging
import os
import pandas as pd
from pathlib import Path
from evaluator import BenchmarkEvaluator, Evaluator
from typing import List, Dict, Any
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
from openai import OpenAI
import ollama
import time
import numpy as np
from datetime import datetime
from collections import defaultdict

class BaseModel:
    def __init__(self, model_name: str):
        self.model_name = model_name
    
    def predict(self, prompt: str) -> str:
        raise NotImplementedError("Subclasses must implement predict method")

class LlamaModel(BaseModel):
    def __init__(self, model_name: str, model_path: str):
        super().__init__(model_name)
        print(f"Loading {model_name}...")
        
        if torch.backends.mps.is_available():
            self.device = "mps"
        elif torch.cuda.is_available():
            self.device = "cuda"
        else:
            self.device = "cpu"
            
        self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
            
        self.model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.float16 if self.device in ["cuda", "mps"] else torch.float32,
            device_map="auto",
            trust_remote_code=True
        )
        print(f"{model_name} loaded successfully!")
    
    def predict(self, prompt: str) -> str:
        try:
            inputs = self.tokenizer(prompt, return_tensors="pt", truncation=True, max_length=2048)
            inputs = {k: v.to(self.model.device) for k, v in inputs.items()}
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=512,
                    temperature=0.7,
                    top_p=0.9,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            response = response[len(prompt):].strip()
            return response
        except Exception as e:
            logging.error(f"Error in {self.model_name} prediction: {e}")
            return ""

class OllamaModel(BaseModel):
    def __init__(self, model_name: str):
        super().__init__(model_name)
        print(f"Initializing Ollama model: {model_name}")
    
    def predict(self, prompt: str) -> str:
        try:
            response = ollama.chat(
                model=self.model_name,
                messages=[{'role': 'user', 'content': prompt}],
                options={'temperature': 0.7}
            )
            return response['message']['content']
        except Exception as e:
            logging.error(f"Ollama Error for {self.model_name}: {str(e)}")
            return ""

class OpenAIModel(BaseModel):
    def __init__(self, model_name: str, api_key: str):
        super().__init__(model_name)
        self.client = OpenAI(api_key=api_key)
        print(f"Initializing OpenAI model: {model_name}")
    
    def predict(self, prompt: str) -> str:
        try:
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.7,
                max_tokens=512
            )
            return response.choices[0].message.content
        except Exception as e:
            logging.error(f"OpenAI Error for {self.model_name}: {str(e)}")
            return ""

def setup_logging():
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    return logging.getLogger(__name__)

def load_benchmark_data(data_path):
    with open(data_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    return data[:15]  # ì²˜ìŒ 15ê°œë§Œ ì‚¬ìš©í•˜ì—¬ ì‹¤í–‰ ì‹œê°„ ë‹¨ì¶•

def measure_latency(model, prompt):
    """Measure response latency for a single prediction"""
    start_time = time.time()
    response = model.predict(prompt)
    end_time = time.time()
    return response, (end_time - start_time) * 1000

def evaluate_model_on_dataset(model, dataset, evaluator, run_number):
    """ë‹¨ì¼ ëª¨ë¸ì„ ë°ì´í„°ì…‹ì—ì„œ í‰ê°€"""
    print(f"  Run {run_number}: Evaluating {model.model_name}...")
    
    predictions = []
    latencies = []
    
    for i, item in enumerate(dataset):
        prompt = f"{item['instruction']}\n\nAnalyze the following logs:\n{json.dumps(item['input'], indent=2)}"
        
        response, latency = measure_latency(model, prompt)
        latencies.append(latency)
        
        predictions.append({
            'id': item.get('id', f'item_{i}'),
            'input': item['input'],
            'prediction': response,
            'expected': item['output'],
            'prompt': prompt
        })
        
        print(f"    Item {i+1}/{len(dataset)} completed (latency: {latency:.2f}ms)")
    
    # í‰ê°€ ìˆ˜í–‰
    try:
        # Evaluator.evaluate ë©”ì„œë“œëŠ” original_data, predictions_dict, dataset_typeì„ ë°›ìŠµë‹ˆë‹¤
        predictions_dict = {model.model_name: predictions}
        results = evaluator.evaluate(dataset, predictions_dict, "attack_detection")
        
        # í•´ë‹¹ ëª¨ë¸ì˜ ë©”íŠ¸ë¦­ ì¶”ì¶œ
        metrics = results.get(model.model_name, {})
        avg_latency = np.mean(latencies)
        
        result = {
            'model': model.model_name,
            'run': run_number,
            'metrics': metrics,
            'avg_latency_ms': avg_latency,
            'total_items': len(predictions)
        }
        
        domain_score = metrics.get('domain_specific_score', 0)
        print(f"    Domain Score: {domain_score:.3f}, Avg Latency: {avg_latency:.2f}ms")
        
        return result
        
    except Exception as e:
        print(f"    Error during evaluation: {e}")
        return {
            'model': model.model_name,
            'run': run_number,
            'metrics': {},
            'avg_latency_ms': np.mean(latencies) if latencies else 0,
            'total_items': len(predictions)
        }

def run_5x_benchmark():
    """5íšŒ ë°˜ë³µ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
    print("ğŸš€ 5íšŒ ë°˜ë³µ ì‚¬ì´ë²„ë³´ì•ˆ ë„ë©”ì¸ LLM ë²¤ì¹˜ë§ˆí¬")
    print("=" * 80)
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
    output_dir = Path("benchmark_results")
    output_dir.mkdir(exist_ok=True)
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
    benchmark_file = Path("test/attack_test_dataset.json")
    if not benchmark_file.exists():
        print(f"âŒ í…ŒìŠ¤íŠ¸ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {benchmark_file}")
        return
    
    benchmark_data = load_benchmark_data(benchmark_file)
    print(f"âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(benchmark_data)}ê°œ ì¼€ì´ìŠ¤")
    
    # ëª¨ë¸ ì„¤ì •
    models = []
    
    try:
        # ì»¤ìŠ¤í…€ ëª¨ë¸ (Llama-PcapLog)
        custom_model = LlamaModel("Llama-PcapLog", "choihyuunmin/Llama-PcapLog")
        models.append(custom_model)
        
        # ë² ì´ìŠ¤ ëª¨ë¸
        base_model = LlamaModel("Meta-Llama-3-8B-Instruct", "meta-llama/Meta-Llama-3-8B-Instruct")
        models.append(base_model)
        
    except Exception as e:
        print(f"âš ï¸ ì¼ë¶€ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        print("Ollama ëª¨ë¸ë“¤ë¡œ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤...")
    
    # Ollama ëª¨ë¸ë“¤ ì¶”ê°€
    ollama_models = ["qwen2:7b", "gemma3:4b", "mistral:7b"]
    for model_name in ollama_models:
        try:
            models.append(OllamaModel(model_name))
        except Exception as e:
            print(f"âš ï¸ {model_name} ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    # OpenAI ëª¨ë¸ ì¶”ê°€ (API í‚¤ê°€ ìˆëŠ” ê²½ìš°)
    if os.getenv("OPENAI_API_KEY"):
        try:
            models.append(OpenAIModel("gpt-4o", os.getenv("OPENAI_API_KEY")))
        except Exception as e:
            print(f"âš ï¸ OpenAI ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    if not models:
        print("âŒ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"ğŸ“Š {len(models)}ê°œ ëª¨ë¸ë¡œ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘:")
    for model in models:
        print(f"  - {model.model_name}")
    
    # í‰ê°€ì ì´ˆê¸°í™”
    evaluator = Evaluator()
    
    # 5íšŒ ë°˜ë³µ ì‹¤í–‰
    all_results = []
    
    for run in range(1, 6):
        print(f"\nğŸ”„ Round {run}/5")
        print("-" * 60)
        
        for model in models:
            try:
                result = evaluate_model_on_dataset(model, benchmark_data, evaluator, run)
                all_results.append(result)
            except Exception as e:
                print(f"âŒ {model.model_name} í‰ê°€ ì‹¤íŒ¨ (Run {run}): {e}")
                continue
    
    # ê²°ê³¼ ë¶„ì„ ë° ì €ì¥
    analyze_and_save_results(all_results, output_dir)

def analyze_and_save_results(all_results, output_dir):
    """ê²°ê³¼ ë¶„ì„ ë° ì €ì¥"""
    print(f"\nğŸ“Š ê²°ê³¼ ë¶„ì„ ì¤‘...")
    
    # ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
    processed_results = []
    
    for result in all_results:
        if result['metrics']:
            row = {
                'model': result['model'],
                'run': result['run'],
                'avg_latency_ms': result['avg_latency_ms'],
                **result['metrics']
            }
            processed_results.append(row)
    
    if not processed_results:
        print("âŒ ë¶„ì„í•  ìˆ˜ ìˆëŠ” ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    df = pd.DataFrame(processed_results)
    
    # ëª¨ë¸ë³„ ìµœê³  ì„±ëŠ¥ ì„ íƒ (5íšŒ ì¤‘ ìµœê³ )
    best_results = []
    for model_name in df['model'].unique():
        model_data = df[df['model'] == model_name]
        if 'domain_specific_score' in model_data.columns:
            best_run = model_data.loc[model_data['domain_specific_score'].idxmax()]
        else:
            best_run = model_data.iloc[0]  # ì²« ë²ˆì§¸ ê²°ê³¼ ì‚¬ìš©
        best_results.append(best_run)
    
    best_df = pd.DataFrame(best_results)
    
    # ìˆœìœ„ ì •ë ¬
    if 'domain_specific_score' in best_df.columns:
        best_df = best_df.sort_values('domain_specific_score', ascending=False)
    
    # ê²°ê³¼ ì €ì¥
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # ì „ì²´ ê²°ê³¼ ì €ì¥
    all_results_path = output_dir / f"all_runs_results_{timestamp}.csv"
    df.to_csv(all_results_path, index=False, encoding='utf-8-sig')
    
    # ìµœê³  ì„±ëŠ¥ ê²°ê³¼ ì €ì¥
    best_results_path = output_dir / f"best_performance_results_{timestamp}.csv"
    best_df.to_csv(best_results_path, index=False, encoding='utf-8-sig')
    
    # ìƒì„¸ JSON ê²°ê³¼ ì €ì¥
    json_results_path = output_dir / f"detailed_results_{timestamp}.json"
    with open(json_results_path, 'w', encoding='utf-8') as f:
        json.dump(all_results, f, indent=2, ensure_ascii=False)
    
    # ì„±ëŠ¥ ë³´ê³ ì„œ ìƒì„±
    generate_performance_report(best_df, output_dir, timestamp)
    
    print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥ ì™„ë£Œ:")
    print(f"  ğŸ“„ ì „ì²´ ê²°ê³¼: {all_results_path}")
    print(f"  ğŸ† ìµœê³  ì„±ëŠ¥: {best_results_path}")
    print(f"  ğŸ“Š ìƒì„¸ JSON: {json_results_path}")

def generate_performance_report(df, output_dir, timestamp):
    """ì„±ëŠ¥ ë³´ê³ ì„œ ìƒì„±"""
    if df.empty:
        return
    
    report = f"""# ğŸš€ 5íšŒ ë°˜ë³µ ì‚¬ì´ë²„ë³´ì•ˆ ë„ë©”ì¸ LLM ë²¤ì¹˜ë§ˆí¬ ë³´ê³ ì„œ

ìƒì„±ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
ì¸¡ì • ë°©ì‹: 5íšŒ ì‹¤í–‰ í›„ ìµœê³  ì„±ëŠ¥ ì„ íƒ

## ğŸ“Š ëª¨ë¸ ì„±ëŠ¥ ìˆœìœ„ (ìµœê³  ì„±ëŠ¥ ê¸°ì¤€)

"""
    
    # ì„±ëŠ¥ ìˆœìœ„ í…Œì´ë¸”
    report += "| ìˆœìœ„ | ëª¨ë¸ëª… | ë„ë©”ì¸ ì ìˆ˜ | ì§€ì—°ì‹œê°„(ms) |\n"
    report += "|------|--------|-------------|---------------|\n"
    
    for i, (_, row) in enumerate(df.iterrows(), 1):
        medal = "ğŸ¥‡" if i == 1 else "ğŸ¥ˆ" if i == 2 else "ğŸ¥‰" if i == 3 else f"{i}ìœ„"
        domain_score = row.get('domain_specific_score', 0)
        latency = row.get('avg_latency_ms', 0)
        report += f"| {medal} | {row['model']} | {domain_score:.3f} | {latency:.2f} |\n"
    
    # ì»¤ìŠ¤í…€ ëª¨ë¸ ì„±ëŠ¥ ë¶„ì„
    if 'Llama-PcapLog' in df['model'].values:
        llama_pcap_row = df[df['model'] == 'Llama-PcapLog'].iloc[0]
        llama_pcap_score = llama_pcap_row.get('domain_specific_score', 0)
        
        report += f"""

## ğŸ¯ Llama-PcapLog ì„±ëŠ¥ ë¶„ì„

### ì£¼ìš” ì„±ê³¼
- **ë„ë©”ì¸ íŠ¹í™” ì ìˆ˜**: {llama_pcap_score:.3f}
- **ìˆœìœ„**: {df[df['model'] == 'Llama-PcapLog'].index[0] + 1}ìœ„
- **í‰ê·  ì§€ì—°ì‹œê°„**: {llama_pcap_row.get('avg_latency_ms', 0):.2f}ms

### ì„¸ë¶€ ì§€í‘œ
"""
        
        # ì„¸ë¶€ ì§€í‘œ ì¶”ê°€
        metrics_to_show = ['attack_classification_accuracy', 'information_extraction_f1', 
                          'threat_detection_accuracy', 'response_quality_score']
        
        for metric in metrics_to_show:
            if metric in llama_pcap_row:
                metric_name = metric.replace('_', ' ').title()
                report += f"- **{metric_name}**: {llama_pcap_row[metric]:.3f}\n"
    
    report += f"""

## ğŸ† ê²°ë¡ 

5íšŒ ë°˜ë³µ ì‹¤í–‰ì„ í†µí•´ ê° ëª¨ë¸ì˜ ìµœê³  ì„±ëŠ¥ì„ ì¸¡ì •í–ˆìŠµë‹ˆë‹¤.
ê²°ê³¼ëŠ” ì‹¤ì œ ëª¨ë¸ í˜¸ì¶œì„ í†µí•´ ì–»ì–´ì§„ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ë°ì´í„°ì…ë‹ˆë‹¤.

---
*ì¸¡ì •ì¼ì‹œ: {datetime.now().strftime('%Yë…„ %mì›” %dì¼ %Hì‹œ %Më¶„')}*
*ì¸¡ì • í™˜ê²½: ì‹¤ì œ ëª¨ë¸ í˜¸ì¶œ ê¸°ë°˜ ë²¤ì¹˜ë§ˆí¬*
"""
    
    # ë³´ê³ ì„œ ì €ì¥
    report_path = output_dir / f"performance_report_{timestamp}.md"
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(f"  ğŸ“‹ ì„±ëŠ¥ ë³´ê³ ì„œ: {report_path}")
    
    # ì½˜ì†”ì—ë„ ìˆœìœ„ ì¶œë ¥
    print(f"\nğŸ† ìµœì¢… ìˆœìœ„:")
    for i, (_, row) in enumerate(df.iterrows(), 1):
        medal = "ğŸ¥‡" if i == 1 else "ğŸ¥ˆ" if i == 2 else "ğŸ¥‰" if i == 3 else f"{i}ìœ„"
        domain_score = row.get('domain_specific_score', 0)
        print(f"  {medal} {row['model']}: {domain_score:.3f}")

def main():
    setup_logging()
    
    print("ğŸš€ 5íšŒ ë°˜ë³µ ì‹¤ì œ ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘")
    print("ì‹¤ì œ Llama-PcapLog ëª¨ë¸ê³¼ ë‹¤ë¥¸ ëª¨ë¸ë“¤ì„ ë¹„êµí•©ë‹ˆë‹¤.")
    print()
    
    try:
        run_5x_benchmark()
        print("\nâœ… ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ!")
        print("ğŸ“ ê²°ê³¼ëŠ” benchmark_results/ í´ë”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
    except KeyboardInterrupt:
        print("\nâš ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        logging.error(f"Benchmark failed: {e}")

if __name__ == '__main__':
    main() 