import argparse
import json
import logging
import os
import pandas as pd
import numpy as np
from pathlib import Path
from evaluator import Evaluator
from typing import List, Dict, Any
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
from openai import OpenAI
import time
import gc
import ast
import re

class BaseModel:
    def __init__(self, model_name: str):
        self.model_name = model_name
    
    def predict(self, prompt: str) -> str:
        raise NotImplementedError("Subclasses must implement predict method")

class LlamaModel(BaseModel):
    def __init__(self, model_name: str, model_path: str):
        super().__init__(model_name)
        if torch.backends.mps.is_available():
            self.device = "mps"
        elif torch.cuda.is_available():
            self.device = "cuda"
        else:
            self.device = "cpu"
        
        print(f"Loading {model_name} on {self.device}...")
        
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(model_path)
            
            # pad_token ì„¤ì •
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # gemma ëª¨ë¸ì˜ ê²½ìš° íŠ¹ë³„í•œ ì„¤ì • ì ìš©
            model_kwargs = {
                "torch_dtype": torch.float16 if self.device in ["cuda", "mps"] else torch.float32,
                "low_cpu_mem_usage": True,
                "trust_remote_code": True
            }
            
            # gemma ëª¨ë¸ì— ëŒ€í•œ íŠ¹ë³„í•œ ì²˜ë¦¬
            if "gemma" in model_name.lower():
                print(f"  Gemma ëª¨ë¸ ê°ì§€: ì•ˆì „í•œ ì„¤ì •ì„ ì ìš©í•©ë‹ˆë‹¤...")
                if self.device == "cuda":
                    model_kwargs.update({
                        "device_map": None,  # auto ëŒ€ì‹  None ì‚¬ìš©
                        "torch_dtype": torch.float32,  # float16 ëŒ€ì‹  float32 ì‚¬ìš©
                    })
                else:
                    model_kwargs["device_map"] = None
            else:
                if self.device == "cuda":
                    model_kwargs["device_map"] = "auto"
                else:
                    model_kwargs["device_map"] = None
            
            self.model = AutoModelForCausalLM.from_pretrained(model_path, **model_kwargs)
            
            # ë””ë°”ì´ìŠ¤ë¡œ ìˆ˜ë™ ì´ë™
            if self.device == "mps":
                self.model = self.model.to("mps")
            elif self.device == "cpu":
                self.model = self.model.to("cpu")
            elif self.device == "cuda" and "gemma" in model_name.lower():
                # gemma ëª¨ë¸ì˜ ê²½ìš° ìˆ˜ë™ìœ¼ë¡œ CUDAë¡œ ì´ë™
                self.model = self.model.to("cuda")
                
            print(f"  {model_name} ë¡œë”© ì™„ë£Œ")
            
        except RuntimeError as e:
            if "CUDA" in str(e) or "device-side assert" in str(e):
                print(f"âš ï¸ {model_name} CUDA ë¡œë”© ì‹¤íŒ¨: {e}")
                print(f"  CPU ëª¨ë“œë¡œ fallback...")
                self.device = "cpu"
                
                # CPU ëª¨ë“œë¡œ ì¬ì‹œë„
                model_kwargs = {
                    "torch_dtype": torch.float32,
                    "device_map": None,
                    "low_cpu_mem_usage": True,
                    "trust_remote_code": True
                }
                
                self.model = AutoModelForCausalLM.from_pretrained(model_path, **model_kwargs)
                self.model = self.model.to("cpu")
                print(f"  {model_name} CPU ëª¨ë“œë¡œ ë¡œë”© ì™„ë£Œ")
            else:
                raise e
    
    def predict(self, prompt: str) -> str:
        try:
            inputs = self.tokenizer(prompt, return_tensors="pt", truncation=True, max_length=2048)
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=512,
                    temperature=0.7,
                    top_p=0.9,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            response = response[len(prompt):].strip()
            return response
            
        except RuntimeError as e:
            if "CUDA" in str(e) or "device-side assert" in str(e):
                print(f"âš ï¸ {self.model_name} ì˜ˆì¸¡ ì¤‘ CUDA ì˜¤ë¥˜: {e}")
                print("  ë” ì‘ì€ ë°°ì¹˜ í¬ê¸°ë¡œ ì¬ì‹œë„í•©ë‹ˆë‹¤...")
                try:
                    # ë” ì‘ì€ í† í° ê¸¸ì´ë¡œ ì¬ì‹œë„
                    inputs = self.tokenizer(prompt, return_tensors="pt", truncation=True, max_length=1024)
                    inputs = {k: v.to(self.device) for k, v in inputs.items()}
                    
                    with torch.no_grad():
                        outputs = self.model.generate(
                            **inputs,
                            max_new_tokens=256,  # ë” ì‘ì€ ì¶œë ¥ ê¸¸ì´
                            temperature=0.7,
                            top_p=0.9,
                            do_sample=True,
                            pad_token_id=self.tokenizer.eos_token_id
                        )
                    
                    response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
                    response = response[len(prompt):].strip()
                    return response
                except Exception as retry_error:
                    print(f"  ì¬ì‹œë„ë„ ì‹¤íŒ¨: {retry_error}")
                    return ""  # ë¹ˆ ë¬¸ìì—´ ë°˜í™˜ìœ¼ë¡œ ìˆ˜ì¹˜ ê³„ì‚° ì˜¤ë¥˜ ë°©ì§€
            else:
                print(f"âš ï¸ {self.model_name} ì˜ˆì¸¡ ì¤‘ ì˜¤ë¥˜: {e}")
                return ""  # ë¹ˆ ë¬¸ìì—´ ë°˜í™˜ìœ¼ë¡œ ìˆ˜ì¹˜ ê³„ì‚° ì˜¤ë¥˜ ë°©ì§€
        except Exception as e:
            print(f"âš ï¸ {self.model_name} ì˜ˆì¸¡ ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜: {e}")
            return ""  # ë¹ˆ ë¬¸ìì—´ ë°˜í™˜ìœ¼ë¡œ ìˆ˜ì¹˜ ê³„ì‚° ì˜¤ë¥˜ ë°©ì§€
    
    def cleanup(self):
        """ë©”ëª¨ë¦¬ ì •ë¦¬"""
        try:
            if hasattr(self, 'model'):
                del self.model
            if hasattr(self, 'tokenizer'):
                del self.tokenizer
            
            # CUDA ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹œ ì˜ˆì™¸ ì²˜ë¦¬
            if torch.cuda.is_available():
                try:
                    torch.cuda.empty_cache()
                    torch.cuda.synchronize()  # GPU ì‘ì—… ì™„ë£Œ ëŒ€ê¸°
                except RuntimeError as e:
                    print(f"âš ï¸ CUDA ë©”ëª¨ë¦¬ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                    print("  ëŒ€ì•ˆì  ë©”ëª¨ë¦¬ ì •ë¦¬ë¥¼ ì‹œë„í•©ë‹ˆë‹¤...")
                    try:
                        # ëŒ€ì•ˆì  ë©”ëª¨ë¦¬ ì •ë¦¬
                        torch.cuda.ipc_collect()
                        gc.collect()
                    except Exception as alt_e:
                        print(f"  ëŒ€ì•ˆì  ë©”ëª¨ë¦¬ ì •ë¦¬ë„ ì‹¤íŒ¨: {alt_e}")
            
            gc.collect()
            print(f"  {self.model_name} ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            print(f"âš ï¸ {self.model_name} ë©”ëª¨ë¦¬ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            print("  ë©”ëª¨ë¦¬ ì •ë¦¬ë¥¼ ê±´ë„ˆë›°ê³  ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")

class OpenAIModel(BaseModel):
    def __init__(self, model_name: str, api_key: str):
        super().__init__(model_name)
        self.api_key = api_key
        self.client = OpenAI(api_key=api_key)
        # ì‹¤ì œ OpenAI API ëª¨ë¸ëª…ìœ¼ë¡œ ë§¤í•‘
        self.model_name = "gpt-4o" if model_name == "gpt-4o" else model_name
        print(f"OpenAI ëª¨ë¸ ì´ˆê¸°í™”: {model_name} (API ëª¨ë¸ëª…: {self.model_name})")
    
    def predict(self, prompt: str) -> str:
        try:
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.7,
                max_tokens=512
            )
            result = response.choices[0].message.content
            return result if result else ""
        except Exception as e:
            print(f"âš ï¸ OpenAI API ì˜¤ë¥˜ ({self.model_name}): {str(e)}")
            return ""  # ë¹ˆ ë¬¸ìì—´ ë°˜í™˜ìœ¼ë¡œ ìˆ˜ì¹˜ ê³„ì‚° ì˜¤ë¥˜ ë°©ì§€
    
    def cleanup(self):
        """OpenAI ëª¨ë¸ ë©”ëª¨ë¦¬ ì •ë¦¬ (API ëª¨ë¸ì´ë¯€ë¡œ íŠ¹ë³„í•œ ì •ë¦¬ ë¶ˆí•„ìš”)"""
        print(f"  {self.model_name} ì •ë¦¬ ì™„ë£Œ (API ëª¨ë¸)")

def setup_logging():
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    return logging.getLogger(__name__)

def load_benchmark_data(data_path):
    with open(data_path, 'r', encoding='utf-8') as f:
        return json.load(f)

def measure_latency(model, prompt):
    """Measure response latency for a single prediction"""
    start_time = time.time()
    response = model.predict(prompt)
    end_time = time.time()
    return response, (end_time - start_time) * 1000  # Convert to milliseconds

def get_code_generation_test_data():
    """Code generation test data - 20 items"""
    return [
        {
            "instruction": "Write a Python function to filter network packet data for connections to a specific port.",
            "input": {"port": 80, "protocol": "tcp"},
            "expected_type": "function"
        },
        {
            "instruction": "Write a Python function that includes regular expressions to extract IP addresses from log data.",
            "input": {"log": "192.168.1.1 - - [25/Dec/2023:10:00:00 +0000] GET /"},
            "expected_type": "function"
        },
        {
            "instruction": "Write a Python class to analyze cyber attack logs and classify attack types.",
            "input": {"attack_types": ["brute_force", "ddos", "malware"]},
            "expected_type": "class"
        },
        {
            "instruction": "Write a Python function to parse firewall rules into JSON format.",
            "input": {"rule": "allow tcp from any to 192.168.1.0/24 port 22"},
            "expected_type": "function"
        },
        {
            "instruction": "Write a function to parse syslog data and convert it into a structured dictionary.",
            "input": {"syslog": "Jan 1 00:00:01 server sshd: Failed password for user"},
            "expected_type": "function"
        },
        {
            "instruction": "Write a packet capture data processing function for network traffic analysis.",
            "input": {"format": "pcap"},
            "expected_type": "function"
        },
        {
            "instruction": "Write a Python class to connect to and query a security event database.",
            "input": {"db_type": "sqlite", "table": "security_events"},
            "expected_type": "class"
        },
        {
            "instruction": "Write a function to detect SQL injection attacks in web logs.",
            "input": {"log_entry": "GET /login.php?id=1' OR '1'='1"},
            "expected_type": "function"
        },
        {
            "instruction": "Write a port scan analysis function for network scan detection.",
            "input": {"connections": [{"src": "192.168.1.100", "dst_port": 22}]},
            "expected_type": "function"
        },
        {
            "instruction": "Write a TLS/SSL connection monitoring function to analyze encrypted communications.",
            "input": {"protocol": "TLS1.3", "cipher": "AES256"},
            "expected_type": "function"
        },
        {
            "instruction": "Write a function to calculate and compare hash values of malware samples.",
            "input": {"hash_type": "sha256", "file_path": "/tmp/sample.exe"},
            "expected_type": "function"
        },
        {
            "instruction": "Write a class to parse Intrusion Detection System (IDS) alerts.",
            "input": {"alert_format": "snort", "severity": "high"},
            "expected_type": "class"
        },
        {
            "instruction": "Write a function to analyze DNS query logs and detect suspicious domains.",
            "input": {"dns_log": "query: evil-domain.com type A"},
            "expected_type": "function"
        },
        {
            "instruction": "Write a function to establish network baselines and detect anomalous traffic.",
            "input": {"baseline_period": "7days", "threshold": 2.0},
            "expected_type": "function"
        },
        {
            "instruction": "Write a class to automatically generate security incident reports.",
            "input": {"template": "NIST", "format": "pdf"},
            "expected_type": "class"
        },
        {
            "instruction": "Write a function to perform container security scanning in virtualized environments.",
            "input": {"container_runtime": "docker", "scan_type": "vulnerability"},
            "expected_type": "function"
        },
        {
            "instruction": "Write a function to collect and analyze cloud security logs.",
            "input": {"cloud_provider": "aws", "service": "cloudtrail"},
            "expected_type": "function"
        },
        {
            "instruction": "Write a class to process real-time threat intelligence feeds.",
            "input": {"feed_type": "IOC", "format": "STIX"},
            "expected_type": "class"
        },
        {
            "instruction": "Write a function to validate network segmentation policies.",
            "input": {"policy": "zero-trust", "network": "192.168.0.0/16"},
            "expected_type": "function"
        },
        {
            "instruction": "Write a class to implement security automation workflows.",
            "input": {"trigger": "security_alert", "action": "isolate_host"},
            "expected_type": "class"
        }
    ]

def is_python_code_valid(code: str) -> bool:
    """Python ì½”ë“œ êµ¬ë¬¸ ìœ íš¨ì„± ê²€ì‚¬"""
    try:
        ast.parse(code)
        return True
    except SyntaxError:
        return False

def extract_python_code(response: str) -> str:
    """ì‘ë‹µì—ì„œ Python ì½”ë“œ ë¸”ë¡ ì¶”ì¶œ"""
    # ```python ì½”ë“œ ë¸”ë¡ ì°¾ê¸°
    python_pattern = r'```python\s*\n(.*?)\n```'
    match = re.search(python_pattern, response, re.DOTALL)
    if match:
        return match.group(1).strip()
    
    # ``` ì¼ë°˜ ì½”ë“œ ë¸”ë¡ ì°¾ê¸°
    code_pattern = r'```\s*\n(.*?)\n```'
    match = re.search(code_pattern, response, re.DOTALL)
    if match:
        return match.group(1).strip()
    
    # def ë˜ëŠ” classë¡œ ì‹œì‘í•˜ëŠ” ë¼ì¸ë“¤ ì°¾ê¸°
    lines = response.split('\n')
    code_lines = []
    in_code = False
    
    for line in lines:
        if line.strip().startswith(('def ', 'class ', 'import ', 'from ')):
            in_code = True
        
        if in_code:
            code_lines.append(line)
            
        # ë¹ˆ ì¤„ì´ ì—°ì†ìœ¼ë¡œ ë‚˜ì˜¤ë©´ ì½”ë“œ ë
        if in_code and line.strip() == '' and len(code_lines) > 1:
            break
    
    return '\n'.join(code_lines).strip()

def evaluate_code_generation_passk(model, test_data, k=3):
    """ì½”ë“œ ìƒì„± pass@k í‰ê°€"""
    total_tasks = len(test_data)
    passed_tasks = 0
    detailed_results = []
    
    for i, task in enumerate(test_data):
        print(f"  ì½”ë“œ ìƒì„± í…ŒìŠ¤íŠ¸ {i+1}/{total_tasks}")
        
        prompt = f"{task['instruction']}\n\nInput: {json.dumps(task['input'], ensure_ascii=False)}\n\nPlease write Python code:"
        
        task_passed = False
        attempts = []
        
        # kë²ˆ ì‹œë„
        for attempt in range(k):
            try:
                response, latency = measure_latency(model, prompt)
                code = extract_python_code(response)
                
                is_valid = is_python_code_valid(code) if code else False
                
                attempts.append({
                    'attempt': attempt + 1,
                    'response': response,
                    'extracted_code': code,
                    'is_valid': is_valid,
                    'latency_ms': latency
                })
                
                if is_valid:
                    task_passed = True
                    break
                    
            except Exception as e:
                print(f"    ì‹œë„ {attempt + 1} ì‹¤íŒ¨: {str(e)}")
                attempts.append({
                    'attempt': attempt + 1,
                    'response': "",  # ë¹ˆ ë¬¸ìì—´ë¡œ ìˆ˜ì •
                    'extracted_code': "",
                    'is_valid': False,
                    'latency_ms': 0
                })
        
        if task_passed:
            passed_tasks += 1
        
        detailed_results.append({
            'task_index': i + 1,
            'instruction': task['instruction'],
            'input_data': json.dumps(task['input'], ensure_ascii=False),
            'passed': task_passed,
            'attempts': attempts,
            'k': k
        })
    
    pass_at_k = passed_tasks / total_tasks if total_tasks > 0 else 0
    
    return {
        'pass_at_k': pass_at_k,
        'passed_tasks': passed_tasks,
        'total_tasks': total_tasks,
        'detailed_results': detailed_results
    }

def test_single_model(model_info, benchmark_data, code_gen_data, evaluator):
    """ë‹¨ì¼ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ë° ë©”ëª¨ë¦¬ ì •ë¦¬"""
    model_name, model_class, model_args = model_info
    
    print(f"\n{'='*60}")
    print(f"Testing {model_name}")
    print(f"{'='*60}")
    
    try:
        # ëª¨ë¸ ë¡œë”© ì‹œ CUDA ê´€ë ¨ ì˜ˆì™¸ ì²˜ë¦¬
        try:
            model = model_class(model_name, model_args)
        except RuntimeError as cuda_error:
            if "CUDA" in str(cuda_error):
                print(f"âš ï¸ {model_name} CUDA ë¡œë”© ì‹¤íŒ¨: {cuda_error}")
                print("  CPU ëª¨ë“œë¡œ ì¬ì‹œë„í•©ë‹ˆë‹¤...")
                # CPU ëª¨ë“œë¡œ fallback ì‹œë„
                original_device = torch.cuda.is_available()
                torch.cuda.is_available = lambda: False  # ì„ì‹œë¡œ CUDA ë¹„í™œì„±í™”
                try:
                    model = model_class(model_name, model_args)
                    print(f"  {model_name} CPU ëª¨ë“œë¡œ ë¡œë”© ì„±ê³µ")
                except Exception as cpu_error:
                    print(f"âŒ {model_name} CPU ëª¨ë“œë¡œë„ ë¡œë”© ì‹¤íŒ¨: {cpu_error}")
                    return [], [], {}
                finally:
                    torch.cuda.is_available = lambda: original_device  # ì›ë˜ ìƒíƒœ ë³µì›
            else:
                raise cuda_error
        
        all_results = []
        detailed_results = []
        test_set_results = {}
        
        for test_type, test_items in benchmark_data.items():
            print(f"\n--- {test_type.upper()} í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ í‰ê°€ ì¤‘ ---")
            print(f"í…ŒìŠ¤íŠ¸ í•­ëª© ìˆ˜: {len(test_items)}")
            
            predictions = []
            latencies = []
            test_set_detailed = []
            
            for item_idx, item in enumerate(test_items):
                try:
                    prompt = f"{item['instruction']}\n\nContext:\n{json.dumps(item['input'], indent=2, ensure_ascii=False)}"
                    
                    response, latency = measure_latency(model, prompt)
                    
                    predictions.append({
                        'input': prompt,
                        'output': response,
                        'expected_output': item['output']
                    })
                    latencies.append(latency)
                    
                    test_item_detail = {
                        'item_index': item_idx + 1,
                        'instruction': item['instruction'],
                        'input_data': json.dumps(item['input'], ensure_ascii=False),
                        'expected_output': item['output'],
                        'generated_output': response,
                        'latency_ms': latency,
                        'response_length': len(response) if response else 0
                    }
                    
                    test_set_detailed.append(test_item_detail)
                    
                    # ì „ì²´ ê²°ê³¼ì—ë„ ì¶”ê°€
                    detailed_results.append({
                        'test_type': test_type,
                        'model': model_name,
                        **test_item_detail
                    })
                    
                    if (item_idx + 1) % 5 == 0:
                        print(f"  ì§„í–‰ë¥ : {item_idx + 1}/{len(test_items)} ì™„ë£Œ")
                        
                except Exception as e:
                    print(f"  ì˜¤ë¥˜ ë°œìƒ (í•­ëª© {item_idx + 1}): {str(e)}")
                    latencies.append(0)
                    
                    error_detail = {
                        'item_index': item_idx + 1,
                        'instruction': item['instruction'],
                        'input_data': json.dumps(item['input'], ensure_ascii=False),
                        'expected_output': item['output'],
                        'generated_output': f"ì˜¤ë¥˜: {str(e)}",
                        'latency_ms': 0,
                        'response_length': 0
                    }
                    
                    test_set_detailed.append(error_detail)
                    detailed_results.append({
                        'test_type': test_type,
                        'model': model_name,
                        **error_detail
                    })
            
            # í‰ê°€ ë©”íŠ¸ë¦­ ê³„ì‚°
            model_predictions = {model_name: predictions}
            try:
                metrics = evaluator.evaluate(test_items, model_predictions, test_type)
                model_metrics = metrics.get(model_name, {})
                
                # ë©”íŠ¸ë¦­ ê°’ ê²€ì¦ ë° ì •ë¦¬ (ìˆ˜ì¹˜ê°€ ì•„ë‹Œ ê°’ë“¤ì„ 0ìœ¼ë¡œ ëŒ€ì²´)
                cleaned_metrics = {}
                for key, value in model_metrics.items():
                    try:
                        # ìˆ«ì í˜•íƒœë¡œ ë³€í™˜ ê°€ëŠ¥í•œì§€ í™•ì¸
                        if isinstance(value, (int, float)):
                            cleaned_metrics[key] = float(value)
                        elif isinstance(value, str) and value.replace('.', '').replace('-', '').isdigit():
                            cleaned_metrics[key] = float(value)
                        else:
                            print(f"  âš ï¸ ë¹„ìˆ˜ì¹˜ ë©”íŠ¸ë¦­ ê°’ ë°œê²¬ ({key}: {value}) -> 0ìœ¼ë¡œ ëŒ€ì²´")
                            cleaned_metrics[key] = 0.0
                    except (ValueError, TypeError):
                        print(f"  âš ï¸ ë©”íŠ¸ë¦­ ë³€í™˜ ì‹¤íŒ¨ ({key}: {value}) -> 0ìœ¼ë¡œ ëŒ€ì²´")
                        cleaned_metrics[key] = 0.0
                        
                model_metrics = cleaned_metrics
                
            except Exception as e:
                print(f"  ë©”íŠ¸ë¦­ ê³„ì‚° ì˜¤ë¥˜: {str(e)}")
                model_metrics = {}
            
            # ê¸°ë³¸ ë©”íŠ¸ë¦­ ì¶”ê°€
            # success_rate: ìœ íš¨í•œ ì¶œë ¥(ë¹ˆ ë¬¸ìì—´ì´ ì•„ë‹Œ)ì„ ìƒì„±í•œ ë¹„ìœ¨
            valid_predictions = len([p for p in predictions if p['output'] and p['output'].strip()])
            model_metrics.update({
                'avg_latency_ms': sum(latencies) / len(latencies) if latencies else 0,
                'total_latency_ms': sum(latencies),
                'total_items': len(test_items),
                'success_rate': valid_predictions / len(predictions) if predictions else 0
            })
            
            # ê²°ê³¼ ì €ì¥
            result = {
                'test_type': test_type,
                'model': model_name,
                **model_metrics
            }
            
            all_results.append(result)
            
            # í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ë³„ ê²°ê³¼ ì €ì¥
            test_set_results[test_type] = {
                'metrics': model_metrics,
                'detailed_results': test_set_detailed,
                'total_items': len(test_items)
            }
            
            # ì£¼ìš” ë©”íŠ¸ë¦­ ì¶œë ¥
            accuracy = model_metrics.get('attack_classification_accuracy', 0)
            avg_latency = model_metrics.get('avg_latency_ms', 0)
            print(f"  ì •í™•ë„: {accuracy:.3f}, í‰ê·  ì§€ì—°ì‹œê°„: {avg_latency:.2f}ms")
        
        # ì½”ë“œ ìƒì„± í…ŒìŠ¤íŠ¸ ìˆ˜í–‰
        print(f"\n--- CODE GENERATION í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ í‰ê°€ ì¤‘ ---")
        print(f"í…ŒìŠ¤íŠ¸ í•­ëª© ìˆ˜: {len(code_gen_data)}")
        
        code_gen_results = evaluate_code_generation_passk(model, code_gen_data, k=3)
        
        # ì½”ë“œ ìƒì„± ê²°ê³¼ë¥¼ ì „ì²´ ê²°ê³¼ì— ì¶”ê°€
        try:
            avg_latency_ms = sum([
                sum([attempt['latency_ms'] for attempt in detail['attempts']])
                for detail in code_gen_results['detailed_results']
            ]) / (len(code_gen_results['detailed_results']) * 3) if code_gen_results['detailed_results'] else 0
        except (ZeroDivisionError, TypeError):
            avg_latency_ms = 0.0
            
        code_gen_result = {
            'test_type': 'code_generation',
            'model': model_name,
            'pass_at_k': float(code_gen_results.get('pass_at_k', 0)),
            'passed_tasks': int(code_gen_results.get('passed_tasks', 0)),
            'total_tasks': int(code_gen_results.get('total_tasks', 0)),
            'avg_latency_ms': float(avg_latency_ms),
            'success_rate': float(code_gen_results.get('pass_at_k', 0))
        }
        
        all_results.append(code_gen_result)
        
        # ì½”ë“œ ìƒì„± í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ë³„ ê²°ê³¼ ì €ì¥
        test_set_results['code_generation'] = {
            'metrics': {
                'pass_at_k': code_gen_results['pass_at_k'],
                'passed_tasks': code_gen_results['passed_tasks'],
                'total_tasks': code_gen_results['total_tasks']
            },
            'detailed_results': code_gen_results['detailed_results'],
            'total_items': len(code_gen_data)
        }
        
        print(f"  Pass@3: {code_gen_results['pass_at_k']:.3f} ({code_gen_results['passed_tasks']}/{code_gen_results['total_tasks']})")
        
        return all_results, detailed_results, test_set_results
        
    except Exception as e:
        print(f"âŒ {model_name} í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
        return [], [], {}

def run_comprehensive_benchmark(benchmark_dir, output_dir):
    """6ê°œ ëª¨ë¸ì— ëŒ€í•œ ì¢…í•© ë²¤ì¹˜ë§ˆí¬ ìˆ˜í–‰ (ìˆœì°¨ ì‹¤í–‰)"""
    print("\n" + "="*80)
    print("COMPREHENSIVE LLM BENCHMARK - 6 MODELS COMPARISON")
    print("="*80)
    
    # 6ê°œ ëª¨ë¸ ì •ì˜ (ê³µê°œ ëª¨ë¸ë“¤ë¡œ ìˆ˜ì •)
    model_configs = [
        ("Llama-PcapLog", LlamaModel, "choihyuunmin/Llama-PcapLog"),
        ("Llama-3-8B", LlamaModel, "meta-llama/Meta-Llama-3-8B"),
        ("Qwen2-7B", LlamaModel, "Qwen/Qwen2-7B"),
        ("Gemma-3-4B-IT", LlamaModel, "google/gemma-3-4b-it"),
        ("Mistral-7B-Instruct", LlamaModel, "mistralai/Mistral-7B-Instruct-v0.1")
    ]
    
    # OpenAI ëª¨ë¸ ì¶”ê°€ (API í‚¤ê°€ ìˆëŠ” ê²½ìš°)
    if os.getenv("OPENAI_API_KEY"):
        model_configs.append(("gpt-4o", OpenAIModel, os.getenv("OPENAI_API_KEY")))
        print("âœ… OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì–´ GPT-4o ëª¨ë¸ì´ í¬í•¨ë©ë‹ˆë‹¤.")
    else:
        print("âš ï¸ OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•„ GPT-4oëŠ” ì œì™¸ë©ë‹ˆë‹¤.")
    
    print(f"ì´ {len(model_configs)}ê°œ ëª¨ë¸ë¡œ ë²¤ì¹˜ë§ˆí¬ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.")
    
    evaluator = Evaluator()
    
    # test_dataset.json íŒŒì¼ ë¡œë“œ
    benchmark_file = benchmark_dir / 'test' / 'test_dataset.json'
    
    if not benchmark_file.exists():
        print(f"Error: {benchmark_file} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
        
    with open(benchmark_file, 'r', encoding='utf-8') as f:
        benchmark_data = json.load(f)
    
    # ì½”ë“œ ìƒì„± í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
    code_gen_data = get_code_generation_test_data()
    
    print(f"ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸: {len(benchmark_data)}ê°œ")
    print(f"ì½”ë“œ ìƒì„± í…ŒìŠ¤íŠ¸: {len(code_gen_data)}ê°œ")
    
    # ì „ì²´ ê²°ê³¼ ì €ì¥ìš©
    all_results = []
    all_detailed_results = []
    all_test_set_results = {}  # ê° í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ë³„ ê²°ê³¼ë“¤
    
    # ê° ëª¨ë¸ì„ ìˆœì°¨ì ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±)
    for i, model_config in enumerate(model_configs):
        model_name = model_config[0]
        print(f"\nëª¨ë¸ {i+1}/{len(model_configs)}: {model_name}")
        
        try:
            # ë‹¨ì¼ ëª¨ë¸ í…ŒìŠ¤íŠ¸
            model_results, model_detailed, model_test_sets = test_single_model(
                model_config, benchmark_data, code_gen_data, evaluator
            )
            
            # ê²°ê³¼ í•©ì¹˜ê¸°
            all_results.extend(model_results)
            all_detailed_results.extend(model_detailed)
            
            for test_type, test_result in model_test_sets.items():
                if test_type not in all_test_set_results:
                    all_test_set_results[test_type] = {}
                all_test_set_results[test_type][model_name] = test_result
            
            print(f"{model_name} í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
            
        except Exception as model_error:
            print(f"âŒ {model_name} ì „ì²´ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {model_error}")
            print(f"  {model_name}ì„(ë¥¼) ê±´ë„ˆë›°ê³  ë‹¤ìŒ ëª¨ë¸ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
            
            # ì‹¤íŒ¨í•œ ëª¨ë¸ì— ëŒ€í•œ ë¹ˆ ê²°ê³¼ ì¶”ê°€ (ì¼ê´€ì„± ìœ ì§€)
            failed_result = {
                'test_type': 'failed',
                'model': model_name,
                'error': str(model_error)[:200],  # ì—ëŸ¬ ë©”ì‹œì§€ ê¸¸ì´ ì œí•œ
                'avg_latency_ms': 0.0,
                'success_rate': 0.0,
                'total_items': 0,
                'passed_tasks': 0,
                'total_tasks': 0
            }
            all_results.append(failed_result)
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬ (CUDA ì—ëŸ¬ ì˜ˆì™¸ ì²˜ë¦¬)
        try:
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
            gc.collect()
            print(f"ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
        except RuntimeError as e:
            print(f"âš ï¸ ë©”ëª¨ë¦¬ ì •ë¦¬ ì¤‘ CUDA ì˜¤ë¥˜ ë°œìƒ: {e}")
            print("  ë©”ëª¨ë¦¬ ì •ë¦¬ë¥¼ ê±´ë„ˆë›°ê³  ë‹¤ìŒ ëª¨ë¸ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
            try:
                gc.collect()  # ìµœì†Œí•œ Python ë©”ëª¨ë¦¬ëŠ” ì •ë¦¬
            except Exception:
                pass
    
    # ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥
    save_benchmark_results_to_csv(all_results, all_detailed_results, all_test_set_results, output_dir)
    
    # ì¢…í•© ìˆœìœ„ ê³„ì‚° ë° ì¶œë ¥
    print_overall_rankings(all_results)
    
    return all_results, all_detailed_results

def clean_data_for_csv(data):
    """CSV ì €ì¥ì„ ìœ„í•œ ë°ì´í„° ì •ë¦¬"""
    if isinstance(data, list):
        return [clean_data_for_csv(item) for item in data]
    elif isinstance(data, dict):
        cleaned = {}
        for key, value in data.items():
            try:
                if isinstance(value, (int, float)):
                    cleaned[key] = float(value) if not pd.isna(value) else 0.0
                elif isinstance(value, str):
                    # ìˆ«ì ë¬¸ìì—´ì¸ì§€ í™•ì¸
                    if value.replace('.', '').replace('-', '').isdigit():
                        cleaned[key] = float(value)
                    else:
                        cleaned[key] = value  # ë¬¸ìì—´ì€ ê·¸ëŒ€ë¡œ ìœ ì§€
                else:
                    cleaned[key] = str(value) if value is not None else ""
            except (ValueError, TypeError):
                cleaned[key] = str(value) if value is not None else ""
        return cleaned
    else:
        return data

def save_benchmark_results_to_csv(results, detailed_results, test_set_results, output_dir):
    """ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥"""
    output_dir = Path(output_dir)
    output_dir.mkdir(exist_ok=True)
    
    # ë°ì´í„° ì •ë¦¬
    cleaned_results = clean_data_for_csv(results)
    cleaned_detailed = clean_data_for_csv(detailed_results)
    
    try:
        # ìš”ì•½ ê²°ê³¼ ì €ì¥
        summary_df = pd.DataFrame(cleaned_results)
        # NaN ê°’ë“¤ì„ 0ìœ¼ë¡œ ëŒ€ì²´
        numeric_columns = summary_df.select_dtypes(include=[np.number]).columns
        summary_df[numeric_columns] = summary_df[numeric_columns].fillna(0)
        
        summary_path = output_dir / 'benchmark_summary.csv'
        summary_df.to_csv(summary_path, index=False, encoding='utf-8-sig')
        print(f"\nìš”ì•½ ê²°ê³¼ê°€ {summary_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        # ìƒì„¸ ê²°ê³¼ ì €ì¥
        detailed_df = pd.DataFrame(cleaned_detailed)
        # NaN ê°’ë“¤ì„ ì ì ˆí•œ ê¸°ë³¸ê°’ìœ¼ë¡œ ëŒ€ì²´
        for col in detailed_df.columns:
            if detailed_df[col].dtype in ['float64', 'int64']:
                detailed_df[col] = detailed_df[col].fillna(0)
            else:
                detailed_df[col] = detailed_df[col].fillna("")
                
        detailed_path = output_dir / 'benchmark_detailed.csv'
        detailed_df.to_csv(detailed_path, index=False, encoding='utf-8-sig')
        print(f"ìƒì„¸ ê²°ê³¼ê°€ {detailed_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
    except Exception as e:
        print(f"âš ï¸ CSV ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print("ë°ì´í„° êµ¬ì¡° í™•ì¸:")
        if results:
            print(f"  results ìƒ˜í”Œ: {results[0] if results else 'None'}")
        if detailed_results:
            print(f"  detailed_results ìƒ˜í”Œ: {detailed_results[0] if detailed_results else 'None'}")
    
    # í†µí•© ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ í…Œì´ë¸” ìƒì„±
    create_unified_benchmark_table(test_set_results, output_dir)
    
    # ê° í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ë³„ ê²°ê³¼ ì €ì¥
    for test_type, test_models in test_set_results.items():
        print(f"\n{test_type} í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ê²°ê³¼ ì €ì¥ ì¤‘...")
        
        # í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ë³„ ìš”ì•½
        test_summary = []
        test_detailed = []
        
        for model_name, model_data in test_models.items():
            # ìš”ì•½ ë°ì´í„°
            summary_row = {
                'model': model_name,
                'test_type': test_type,
                **model_data['metrics']
            }
            test_summary.append(summary_row)
            
            # ìƒì„¸ ë°ì´í„°
            for detail in model_data['detailed_results']:
                detail_row = {
                    'model': model_name,
                    'test_type': test_type,
                    **detail
                }
                test_detailed.append(detail_row)
        
        # í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ë³„ ìš”ì•½ ì €ì¥
        if test_summary:
            test_summary_df = pd.DataFrame(test_summary)
            test_summary_path = output_dir / f'{test_type}_summary.csv'
            test_summary_df.to_csv(test_summary_path, index=False, encoding='utf-8-sig')
            print(f"  {test_type} ìš”ì•½: {test_summary_path}")
        
    
    # ëª¨ë¸ë³„ í‰ê·  ì„±ëŠ¥ ê³„ì‚° (ëª¨ë“  í…ŒìŠ¤íŠ¸ í¬í•¨)
    if results:
        # ê³µê²© ë¶„ë¥˜ ì •í™•ë„ê°€ ìˆëŠ” ê²°ê³¼ë§Œ í•„í„°ë§
        attack_results = [r for r in results if 'attack_classification_accuracy' in r]
        if attack_results:
            attack_df = pd.DataFrame(attack_results)
            model_performance = attack_df.groupby('model').agg({
                'attack_classification_accuracy': 'mean',
                'avg_latency_ms': 'mean',
                'success_rate': 'mean'
            }).round(4)
            
            performance_path = output_dir / 'model_performance_comparison.csv'
            model_performance.to_csv(performance_path, encoding='utf-8-sig')
            print(f"ëª¨ë¸ ì„±ëŠ¥ ë¹„êµê°€ {performance_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        # Pass@k ê²°ê³¼ë§Œ ë”°ë¡œ ì €ì¥
        passk_results = [r for r in results if 'pass_at_k' in r]
        if passk_results:
            passk_df = pd.DataFrame(passk_results)
            passk_path = output_dir / 'code_generation_passk.csv'
            passk_df.to_csv(passk_path, index=False, encoding='utf-8-sig')
            print(f"ì½”ë“œ ìƒì„± Pass@k ê²°ê³¼ê°€ {passk_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

def create_unified_benchmark_table(test_set_results, output_dir):
    """ëª¨ë“  ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ë¥¼ í•˜ë‚˜ì˜ í†µí•©ëœ í…Œì´ë¸”ë¡œ ìƒì„±"""
    unified_results = []
    
    # ê° ëª¨ë¸ê³¼ í…ŒìŠ¤íŠ¸ íƒ€ì…ì— ëŒ€í•´ í†µí•© í–‰ ìƒì„±
    all_models = set()
    for test_type, test_models in test_set_results.items():
        all_models.update(test_models.keys())
    
    for model in all_models:
        for test_type, test_models in test_set_results.items():
            if model in test_models:
                metrics = test_models[model]['metrics']
                
                # ê¸°ë³¸ ì •ë³´
                row = {
                    'model': model,
                    'type': test_type,
                }
                
                # ëª¨ë“  ë©”íŠ¸ë¦­ ì¶”ê°€
                metric_mapping = {
                    # ê³µê²© ë¶„ë¥˜ ë©”íŠ¸ë¦­
                    'attack_classification_accuracy': 'attack_accuracy',
                    'attack_classification_f1': 'attack_f1',
                    'attack_classification_precision': 'attack_precision',
                    'attack_classification_recall': 'attack_recall',
                    
                    # ì •ë³´ ì¶”ì¶œ ë©”íŠ¸ë¦­
                    'ip_extraction_f1': 'ip_f1',
                    'port_extraction_f1': 'port_f1',
                    'protocol_extraction_f1': 'protocol_f1',
                    'overall_extraction_f1': 'extraction_f1',
                    
                    # ìœ„í˜‘ íƒì§€ ë©”íŠ¸ë¦­
                    'threat_detection_accuracy': 'threat_accuracy',
                    'threat_detection_f1': 'threat_f1',
                    'threat_detection_precision': 'threat_precision',
                    'threat_detection_recall': 'threat_recall',
                    
                    # ìì—°ì–´ ìƒì„± í’ˆì§ˆ ë©”íŠ¸ë¦­
                    'bleu': 'bleu_score',
                    'rouge_l': 'rouge_l_score',
                    'semantic_similarity': 'cosine_similarity',
                    'response_quality_score': 'response_quality',
                    
                    # ë„ë©”ì¸ íŠ¹í™” ë©”íŠ¸ë¦­
                    'domain_specific_score': 'domain_score',
                    'overall_score': 'overall_score',
                    
                    # ì½”ë“œ ìƒì„± ë©”íŠ¸ë¦­
                    'pass_at_k': 'pass_at_3',
                    'passed_tasks': 'passed_tasks',
                    'total_tasks': 'total_tasks',
                    
                    # ì„±ëŠ¥ ë©”íŠ¸ë¦­
                    'avg_latency_ms': 'avg_latency_ms',
                    'total_latency_ms': 'total_latency_ms',
                    'success_rate': 'success_rate',
                    'total_items': 'total_items'
                }
                
                # ëª¨ë“  ê°€ëŠ¥í•œ ë©”íŠ¸ë¦­ì„ ê¸°ë³¸ê°’ 0ìœ¼ë¡œ ì´ˆê¸°í™”
                for original_key, new_key in metric_mapping.items():
                    row[new_key] = metrics.get(original_key, 0)
                
                # íŠ¹ë³„ ì²˜ë¦¬: ì½”ë“œ ìƒì„±ì´ ì•„ë‹Œ ê²½ìš° pass@k ê´€ë ¨ í•„ë“œëŠ” nullë¡œ ì„¤ì •
                if test_type != 'code_generation':
                    row['pass_at_3'] = None
                    row['passed_tasks'] = None
                    row['total_tasks'] = None
                
                # íŠ¹ë³„ ì²˜ë¦¬: ì½”ë“œ ìƒì„±ì¸ ê²½ìš° ë‹¤ë¥¸ ë©”íŠ¸ë¦­ë“¤ì€ nullë¡œ ì„¤ì •
                elif test_type == 'code_generation':
                    non_code_metrics = [
                        'attack_accuracy', 'attack_f1', 'attack_precision', 'attack_recall',
                        'ip_f1', 'port_f1', 'protocol_f1', 'extraction_f1',
                        'threat_accuracy', 'threat_f1', 'threat_precision', 'threat_recall',
                        'bleu_score', 'rouge_l_score', 'cosine_similarity', 'response_quality',
                        'domain_score', 'overall_score'
                    ]
                    for metric in non_code_metrics:
                        row[metric] = None
                
                unified_results.append(row)
    
    # DataFrame ìƒì„± ë° ì €ì¥
    if unified_results:
        unified_df = pd.DataFrame(unified_results)
        
        # ì»¬ëŸ¼ ìˆœì„œ ì •ë¦¬
        column_order = [
            'model', 'type',
            'attack_accuracy', 'attack_f1', 'attack_precision', 'attack_recall',
            'ip_f1', 'port_f1', 'protocol_f1', 'extraction_f1',
            'threat_accuracy', 'threat_f1', 'threat_precision', 'threat_recall',
            'bleu_score', 'rouge_l_score', 'cosine_similarity', 'response_quality',
            'domain_score', 'overall_score',
            'pass_at_3', 'passed_tasks', 'total_tasks',
            'avg_latency_ms', 'success_rate', 'total_items'
        ]
        
        # ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ ì„ íƒ
        available_columns = [col for col in column_order if col in unified_df.columns]
        unified_df = unified_df[available_columns]
        
        # ëª¨ë¸ëª…ìœ¼ë¡œ ì •ë ¬ (Llama-PcapLogê°€ ë§¨ ìœ„ì— ì˜¤ë„ë¡)
        unified_df['sort_key'] = unified_df['model'].apply(lambda x: 0 if x == 'Llama-PcapLog' else 1)
        unified_df = unified_df.sort_values(['sort_key', 'model', 'type']).drop('sort_key', axis=1)
        
        # ì†Œìˆ˜ì  ë°˜ì˜¬ë¦¼
        numeric_columns = unified_df.select_dtypes(include=[float]).columns
        unified_df[numeric_columns] = unified_df[numeric_columns].round(4)
        
        # ì €ì¥
        unified_path = output_dir / 'unified_benchmark_results.csv'
        unified_df.to_csv(unified_path, index=False, encoding='utf-8-sig')
        print(f"\nğŸ¯ í†µí•© ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ê°€ {unified_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print(f"   - ì»¬ëŸ¼: {len(unified_df.columns)}ê°œ")
        print(f"   - í–‰: {len(unified_df)}ê°œ")
        print(f"   - í¬í•¨ëœ ë©”íŠ¸ë¦­: attack_accuracy, f1_scores, cosine_similarity, pass@3 ë“±")

def print_overall_rankings(results):
    """ì¢…í•© ìˆœìœ„ ì¶œë ¥"""
    print("\n" + "="*80)
    print("ì¢…í•© ì„±ëŠ¥ ìˆœìœ„")
    print("="*80)
    
    if not results:
        print("ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ê³µê²© ë¶„ë¥˜ ì •í™•ë„ ê¸°ì¤€ ìˆœìœ„
    attack_results = [r for r in results if 'attack_classification_accuracy' in r and r['test_type'] != 'code_generation']
    if attack_results:
        from collections import defaultdict
        model_stats = defaultdict(list)
        
        for result in attack_results:
            model_stats[result['model']].append(result['attack_classification_accuracy'])
        
        model_averages = {model: sum(scores)/len(scores) for model, scores in model_stats.items()}
        sorted_models = sorted(model_averages.items(), key=lambda x: x[1], reverse=True)
        
        print("\ní‰ê·  ì •í™•ë„ ê¸°ì¤€ ìˆœìœ„ (ê³µê²© íƒì§€):")
        for rank, (model, avg_accuracy) in enumerate(sorted_models, 1):
            if model == "Llama-PcapLog":
                print(f"{rank}ìœ„: {model} - {avg_accuracy:.4f} (ìš°ë¦¬ ëª¨ë¸!)")
            else:
                print(f"{rank}ìœ„: {model} - {avg_accuracy:.4f}")
        
        # Llama-PcapLog ëª¨ë¸ì˜ ì„±ëŠ¥ ë¶„ì„
        llama_pcaplog_rank = next((i for i, (model, _) in enumerate(sorted_models, 1) if model == "Llama-PcapLog"), None)
        if llama_pcaplog_rank:
            llama_score = model_averages["Llama-PcapLog"]
    
    # ì½”ë“œ ìƒì„± Pass@k ìˆœìœ„
    passk_results = [r for r in results if 'pass_at_k' in r]
    if passk_results:
        print(f"\nì½”ë“œ ìƒì„± Pass@3 ìˆœìœ„:")
        passk_sorted = sorted(passk_results, key=lambda x: x['pass_at_k'], reverse=True)
        
        for rank, result in enumerate(passk_sorted, 1):
            model = result['model']
            passk = result['pass_at_k']
            passed = result['passed_tasks']
            total = result['total_tasks']
            
            if model == "Llama-PcapLog":
                print(f"{rank}ìœ„: {model} - {passk:.3f} ({passed}/{total}) (ìš°ë¦¬ ëª¨ë¸!)")
            else:
                print(f"{rank}ìœ„: {model} - {passk:.3f} ({passed}/{total})")

def main():
    parser = argparse.ArgumentParser(description='Run comprehensive LLM benchmark')
    parser.add_argument('--benchmark_dir', type=str, default='.',
                       help='Directory containing benchmark data')
    parser.add_argument('--output_dir', type=str, default='results',
                       help='Directory to save results')
    
    args = parser.parse_args()
    
    benchmark_dir = Path(args.benchmark_dir)
    output_dir = Path(args.output_dir)
    output_dir.mkdir(exist_ok=True)
    
    setup_logging()
    
    print("6ê°œ ëª¨ë¸ ì¢…í•© ë²¤ì¹˜ë§ˆí¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    print("ëª¨ë¸ ëª©ë¡:")
    print("1. Llama-PcapLog (ìš°ë¦¬ ëª¨ë¸)")
    print("2. Llama-3.1-8B-Instruct")
    print("3. Qwen2-7B") 
    print("4. Gemma-3-4B-IT")
    print("5. Mistral-7B-Instruct")
    print("6. GPT-4o (API í‚¤ê°€ ìˆëŠ” ê²½ìš°)")
    
    print("\ní…ŒìŠ¤íŠ¸ êµ¬ì„±:")
    print("- ê¸°ë³¸ ë¶„ì„ í…ŒìŠ¤íŠ¸: 20ê°œ")
    print("- ê³ ê¸‰ ë¶„ì„ í…ŒìŠ¤íŠ¸: 20ê°œ") 
    print("- ìœ„í˜‘ íƒì§€ í…ŒìŠ¤íŠ¸: 20ê°œ")
    print("- ì½”ë“œ ìƒì„± í…ŒìŠ¤íŠ¸: 20ê°œ (Pass@3)")
    print("- ì´ 80ê°œ í…ŒìŠ¤íŠ¸")
    
    # ì¢…í•© ë²¤ì¹˜ë§ˆí¬ ìˆ˜í–‰
    results, detailed_results = run_comprehensive_benchmark(benchmark_dir, output_dir)
    
    print(f"\nëª¨ë“  ë²¤ì¹˜ë§ˆí¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ê²°ê³¼ëŠ” {output_dir} í´ë”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == '__main__':
    main() 
